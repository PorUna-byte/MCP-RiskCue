#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Pipeline for automated model evaluation across different LLM models.
"""

import os
import sys
import subprocess
import time
import json
from pathlib import Path
from typing import List, Dict, Optional
import argparse
from dotenv import load_dotenv

# è®¾ç½®ç¯å¢ƒå˜é‡ä»¥é¿å… FX ç¬¦å·è¿½è¸ªé—®é¢˜
os.environ["TORCH_COMPILE_DEBUG"] = "0"
os.environ["TORCH_LOGS"] = "-dynamo"
os.environ["TORCH_DYNAMO_DISABLE"] = "1"  # å®Œå…¨ç¦ç”¨ dynamo
os.environ["TORCH_COMPILE_DISABLE"] = "1"  # ç¦ç”¨ torch.compile
os.environ["TRANSFORMERS_VERBOSITY"] = "error"  # å‡å°‘ transformers æ—¥å¿—

# é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = Path(__file__).resolve().parent.parent   

MODEL_INFO = {
    #API models
    "claude-3-7-sonnet-20250219": {
        "local": False,
    },
    "doubao-1-5-pro-32k-250115": {
        "local": False,
    },
    "gemini-2.5-pro": {
        "local": False,
    },
    "glm-4.5v": {
        "local": False,
    },
    "grok-4": {
        "local": False,
    },
    "gpt-4o": {
        "local": False,
    },
    "gpt-5-2025-08-07": {
        "local": False,
    },
    "deepseek-r1": {
        "local": False,
    },
    "kimi-k2-0711-preview": {
        "local": False,
    },
    "o3-2025-04-16": {
        "local": False,
    },
    # æœ¬åœ°æ¨¡å‹
    "Qwen3-4B-Instruct": {
        "local": True,
        "model_path": "/mnt/data-alpha-sg-02/team-agent/s00513066/checkpoints_mcp/Qwen3-4B-Instruct-2507",
        "tokenizer_path": "/mnt/data-alpha-sg-02/team-agent/s00513066/checkpoints_mcp/Qwen3-4B-Instruct-2507",
    },
    "sft_Qwen3-4B-Instruct": {
        "local": True,
        "model_path": "/mnt/data-alpha-sg-02/team-agent/s00513066/checkpoints_mcp/sft_Qwen3-4B-Instruct-2507",
        "tokenizer_path": "/mnt/data-alpha-sg-02/team-agent/s00513066/checkpoints_mcp/Qwen3-4B-Instruct-2507",
    },
    "grpo_Qwen3-4B-Instruct": {
        "local": True,
        "model_path": "/mnt/data-alpha-sg-02/team-agent/s00513066/checkpoints_mcp/grpo_Qwen3-4B-Instruct-2507",
        "tokenizer_path": "/mnt/data-alpha-sg-02/team-agent/s00513066/checkpoints_mcp/Qwen3-4B-Instruct-2507",
    },
    
    "Llama3.1-8B-Instruct": {
        "local": True,
        "model_path": "/mnt/data-alpha-sg-02/team-agent/s00513066/checkpoints_mcp/Llama-3.1-8B-Instruct",
        "tokenizer_path": "/mnt/data-alpha-sg-02/team-agent/s00513066/checkpoints_mcp/Llama-3.1-8B-Instruct",
    },
    "sft_Llama3.1-8B-Instruct": {
        "local": True,
        "model_path": "/mnt/data-alpha-sg-02/team-agent/s00513066/checkpoints_mcp/sft_Llama-3.1-8B-Instruct",
        "tokenizer_path": "/mnt/data-alpha-sg-02/team-agent/s00513066/checkpoints_mcp/Llama-3.1-8B-Instruct",
    },
    "grpo_Llama3.1-8B-Instruct": {
        "local": True,
        "model_path": "/mnt/data-alpha-sg-02/team-agent/s00513066/checkpoints_mcp/grpo_Llama-3.1-8B-Instruct",
        "tokenizer_path": "/mnt/data-alpha-sg-02/team-agent/s00513066/checkpoints_mcp/Llama-3.1-8B-Instruct",
    },  
    
    "DeepSeek-R1-0528-Qwen3-8B": {
        "local": True,
        "model_path": "/mnt/data-alpha-sg-02/team-agent/s00513066/checkpoints_mcp/DeepSeek-R1-0528-Qwen3-8B",
        "tokenizer_path": "/mnt/data-alpha-sg-02/team-agent/s00513066/checkpoints_mcp/DeepSeek-R1-0528-Qwen3-8B",
    },
    "sft_DeepSeek-R1-0528-Qwen3-8B": {
        "local": True,
        "model_path": "/mnt/data-alpha-sg-02/team-agent/s00513066/checkpoints_mcp/sft_DeepSeek-R1-0528-Qwen3-8B",
        "tokenizer_path": "/mnt/data-alpha-sg-02/team-agent/s00513066/checkpoints_mcp/DeepSeek-R1-0528-Qwen3-8B",
    },
    "grpo_DeepSeek-R1-0528-Qwen3-8B": {
        "local": True,
        "model_path": "/mnt/data-alpha-sg-02/team-agent/s00513066/checkpoints_mcp/grpo_DeepSeek-R1-0528-Qwen3-8B",
        "tokenizer_path": "/mnt/data-alpha-sg-02/team-agent/s00513066/checkpoints_mcp/DeepSeek-R1-0528-Qwen3-8B",
    },
    "Qwen3-4B-Thinking-2507":{
        "local": True,
        "model_path": "/mnt/data-alpha-sg-02/team-agent/s00513066/checkpoints_mcp/Qwen3-4B-Thinking-2507",
        "tokenizer_path": "/mnt/data-alpha-sg-02/team-agent/s00513066/checkpoints_mcp/Qwen3-4B-Thinking-2507",
    },
    "sft_Qwen3-4B-Thinking-2507":{
        "local": True,
        "model_path": "/mnt/data-alpha-sg-02/team-agent/s00513066/checkpoints_mcp/sft_Qwen3-4B-Thinking-2507",
        "tokenizer_path": "/mnt/data-alpha-sg-02/team-agent/s00513066/checkpoints_mcp/Qwen3-4B-Thinking-2507",
    },
    "grpo_Qwen3-4B-Thinking-2507":{
        "local": True,
        "model_path": "/mnt/data-alpha-sg-02/team-agent/s00513066/checkpoints_mcp/grpo_Qwen3-4B-Thinking-2507",
        "tokenizer_path": "/mnt/data-alpha-sg-02/team-agent/s00513066/checkpoints_mcp/Qwen3-4B-Thinking-2507",
    },
    
    "Qwen3Guard-Gen-4B":{
        "local": True,
        "model_path": "/mnt/data-alpha-sg-02/team-agent/s00513066/checkpoints_mcp/Qwen3Guard-Gen-4B",
        "tokenizer_path": "/mnt/data-alpha-sg-02/team-agent/s00513066/checkpoints_mcp/Qwen3-4B",
    },
    "sft_Qwen3Guard-Gen-4B":{
        "local": True,
        "model_path": "/mnt/data-alpha-sg-02/team-agent/s00513066/checkpoints_mcp/sft_Qwen3Guard-Gen-4B",
        "tokenizer_path": "/mnt/data-alpha-sg-02/team-agent/s00513066/checkpoints_mcp/Qwen3-4B",
    },
    "grpo_Qwen3Guard-Gen-4B":{
        "local": True,
        "model_path": "/mnt/data-alpha-sg-02/team-agent/s00513066/checkpoints_mcp/grpo_Qwen3Guard-Gen-4B",
        "tokenizer_path": "/mnt/data-alpha-sg-02/team-agent/s00513066/checkpoints_mcp/Qwen3-4B",
    },
    "Llama-Guard-3-8B":{
        "local": True,
        "model_path": "/mnt/data-alpha-sg-02/team-agent/s00513066/checkpoints_mcp/Llama-Guard-3-8B",
        "tokenizer_path": "/mnt/data-alpha-sg-02/team-agent/s00513066/checkpoints_mcp/Llama-3.1-8B-Instruct",
    },
    "sft_Llama-Guard-3-8B":{
        "local": True,
        "model_path": "/mnt/data-alpha-sg-02/team-agent/s00513066/checkpoints_mcp/sft_Llama-Guard-3-8B",
        "tokenizer_path": "/mnt/data-alpha-sg-02/team-agent/s00513066/checkpoints_mcp/Llama-3.1-8B-Instruct",
    },
    "grpo_Llama-Guard-3-8B":{
        "local": True,
        "model_path": "/mnt/data-alpha-sg-02/team-agent/s00513066/checkpoints_mcp/grpo_Llama-Guard-3-8B",
        "tokenizer_path": "/mnt/data-alpha-sg-02/team-agent/s00513066/checkpoints_mcp/Llama-3.1-8B-Instruct",
    },
}
# é»˜è®¤è¦å¤„ç†çš„æ¨¡å‹åˆ—è¡¨ï¼ˆå¯ä»¥ä»MODEL_INFOä¸­é€‰æ‹©ï¼‰
DEFAULT_MODELS = [
    "claude-3-7-sonnet-20250219",  
    "doubao-1-5-pro-32k-250115",
    "gemini-2.5-pro", 
    "glm-4.5v", 
    "grok-4", 
    "gpt-4o",
    "gpt-5-2025-08-07",
    "deepseek-r1",
    "kimi-k2-0711-preview",
    "o3-2025-04-16",
]

# æ•°æ®æ–‡ä»¶è·¯å¾„
DATA_FILES = {
    "prin": PROJECT_ROOT / "Data" / "prin_data_test.jsonl",
    "env": PROJECT_ROOT / "Data" / "env_data_test.jsonl"
}

# ç³»ç»Ÿæç¤ºæ–‡ä»¶è·¯å¾„
SYSTEM_PROMPTS = {
    "prin": PROJECT_ROOT / "Prompts" / "sys_prompt_prin.txt",
    "env": PROJECT_ROOT / "Prompts" / "sys_prompt_env.txt"
}

# è¾“å‡ºç›®å½•ç»“æ„
OUTPUT_DIR = Path(__file__).resolve().parent
HISTORY_DIR = OUTPUT_DIR / "history" / "remote"
EVALUATION_DIR = OUTPUT_DIR / "evaluation" / "remote"

# åˆ›å»ºå¿…è¦çš„ç›®å½•
HISTORY_DIR.mkdir(exist_ok=True)
EVALUATION_DIR.mkdir(exist_ok=True)

class ProgressMonitor:
    """ç®€åŒ–çš„è¿›åº¦ç›‘æ§å™¨"""
    
    def __init__(self, total_models: int):
        self.total_models = total_models
        self.current_model = 0
        self.start_time = time.time()
        
    def update_model(self, model_name: str):
        """æ›´æ–°å½“å‰å¤„ç†çš„æ¨¡å‹"""
        self.current_model += 1
        total_elapsed = time.time() - self.start_time
        
        print(f"\n{'='*60}")
        print(f"ğŸš€ Processing model {self.current_model}/{self.total_models}: {model_name}")
        print(f"â±ï¸  Total time: {total_elapsed:.1f}s")
        
        if self.current_model > 0:
            avg_time = total_elapsed / self.current_model
            remaining = self.total_models - self.current_model
            estimated = remaining * avg_time
            print(f"ğŸ“ˆ Estimated remaining: {estimated/3600:.1f}h")
        
        print(f"{'='*60}")

def load_dotenv_safe():
    """å®‰å…¨åœ°åŠ è½½.envæ–‡ä»¶"""
    try:
        load_dotenv()
        print("âœ“ .env file loaded successfully")
    except Exception as e:
        print(f"âš  Warning: Could not load .env file: {e}")

def get_available_models():
    """è·å–æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨"""
    return list(MODEL_INFO.keys())

def get_local_models():
    """è·å–æ‰€æœ‰æœ¬åœ°æ¨¡å‹åˆ—è¡¨"""
    return [model for model, info in MODEL_INFO.items() if info["local"]]

def get_online_models():
    """è·å–æ‰€æœ‰åœ¨çº¿æ¨¡å‹åˆ—è¡¨"""
    return [model for model, info in MODEL_INFO.items() if not info["local"]]

def update_env_model(model: str):
    """æ›´æ–°.envæ–‡ä»¶ä¸­çš„æ¨¡å‹ç›¸å…³é…ç½®"""
    env_file = PROJECT_ROOT / ".env"
    if not env_file.exists():
        print(f"âŒ .env file not found at {env_file}")
        return False
    
    try:
        with open(env_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        # ä»MODEL_INFOä¸­è·å–æ¨¡å‹ä¿¡æ¯
        if model not in MODEL_INFO:
            print(f"âŒ Model {model} not found in MODEL_INFO")
            return False
        
        model_info = MODEL_INFO[model]
        local_value = str(model_info["local"])
        local_model_path_value = model_info.get("model_path", None)
        local_tokenizer_path_value = model_info.get("tokenizer_path", None)
        
        
        # éœ€è¦æ›´æ–°çš„ç¯å¢ƒå˜é‡
        env_updates = {
            'MODEL': f'"{model}"',
            'LOCAL': f'"{local_value}"',
            'LOCAL_MODEL_PATH': f'"{local_model_path_value}"' if local_model_path_value else '""',
            'LOCAL_TOKENIZER_PATH': f'"{local_tokenizer_path_value}"' if local_tokenizer_path_value else '""',
        }
        
        # é‡å†™æ•´ä¸ªæ–‡ä»¶ï¼Œé¿å…é‡å¤è¡Œ
        new_lines = []
        updated_vars = set()
        
        # å¤„ç†ç°æœ‰è¡Œ
        for line in lines:
            line_stripped = line.strip()
            line_updated = False
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æˆ‘ä»¬è¦æ›´æ–°çš„ç¯å¢ƒå˜é‡
            for var_name, var_value in env_updates.items():
                if (line_stripped.startswith(var_name + ' =') or 
                    line_stripped.startswith(var_name + '=')):
                    if var_name not in updated_vars:
                        new_lines.append(f'{var_name} = {var_value}\n')
                        updated_vars.add(var_name)
                        line_updated = True
                    break
            
            # å¦‚æœä¸æ˜¯æˆ‘ä»¬è¦æ›´æ–°çš„ç¯å¢ƒå˜é‡ï¼Œä¿ç•™åŸè¡Œ
            if not line_updated:
                new_lines.append(line)
        
        # æ·»åŠ æœªæ‰¾åˆ°çš„ç¯å¢ƒå˜é‡
        for var_name, var_value in env_updates.items():
            if var_name not in updated_vars:
                new_lines.append(f'{var_name} = {var_value}\n')
        
        lines = new_lines
        
        with open(env_file, 'w', encoding='utf-8') as f:
            f.writelines(lines)
            f.flush()
        
        # é‡æ–°åŠ è½½ç¯å¢ƒå˜é‡
        load_dotenv(override=True)
        
        print(f"âœ“ Updated .env with:")
        print(f"   MODEL = {model}")
        print(f"   LOCAL = {local_value}")
        print(f"   LOCAL_MODEL_PATH = {local_model_path_value or 'None'}")
        print(f"   LOCAL_TOKENIZER_PATH = {local_tokenizer_path_value or 'None'}")
        return True
        
    except Exception as e:
        print(f"âŒ Failed to update .env file: {e}")
        return False

def run_history_generation(data_type: str, max_workers: int = 10, debug: bool = False):
    """è¿è¡Œhistoryç”Ÿæˆï¼Œå¸¦å®æ—¶è¿›åº¦æ˜¾ç¤º"""
    data_file = DATA_FILES[data_type]
    system_prompt = SYSTEM_PROMPTS[data_type]
    

    server_category = "Env_risk"
    output_file = HISTORY_DIR / f"histories_env_{os.getenv('MODEL', 'eval')}.jsonl"
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    output_file.parent.mkdir(parents=True, exist_ok=True)
    
    if not data_file.exists():
        print(f"âŒ Data file not found: {data_file}")
        return False
    
    if not system_prompt.exists():
        print(f"âŒ System prompt file not found: {system_prompt}")
        return False
    
    print(f"ğŸ”„ Generating history for {data_type} data...")
    print(f"   ğŸ“ Input: {data_file.name}")
    print(f"   ğŸ“ Output: {output_file.name}")
    print(f"   ğŸ”§ Workers: {max_workers}")
    
    # è·å–è¾“å…¥æ–‡ä»¶çš„è¡Œæ•°ï¼ˆä¼°ç®—æ€»ä»»åŠ¡æ•°ï¼‰
    try:
        with open(data_file, 'r', encoding='utf-8') as f:
            total_lines = sum(1 for _ in f)
        print(f"   ğŸ“Š Estimated total queries: {total_lines}")
    except Exception:
        total_lines = 0
        print(f"   âš  Could not determine total queries")
    
    cmd = [
        sys.executable, str(PROJECT_ROOT / "Data" / "history_generator.py"),
        "--query-file", str(data_file),
        "--resp-file", str(output_file),
        "--system-prompt", str(system_prompt),
        "--server_category", server_category,
        "--max-workers", str(max_workers)
    ]
    
    if debug:
        cmd.append("--debug")
    
    try:
        # å¯åŠ¨è¿›ç¨‹å¹¶å®æ—¶æ˜¾ç¤ºè¾“å‡º
        process = subprocess.Popen(
            cmd,
            cwd=PROJECT_ROOT,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True,
            bufsize=1,
            universal_newlines=True
        )
        
        # å®æ—¶è¯»å–è¾“å‡ºå¹¶æ˜¾ç¤ºè¿›åº¦
        start_time = time.time()
        last_progress_time = start_time
        last_file_size = 0
        last_file_check_time = start_time
        
        while True:
            output = process.stdout.readline()
            if output == '' and process.poll() is not None:
                break
            if output:
                output = output.strip()
                
                # æ˜¾ç¤ºå…³é”®è¿›åº¦ä¿¡æ¯
                if any(keyword in output.lower() for keyword in [
                    'completed', 'failed', 'progress', 'processed', 'queries'
                ]):
                    current_time = time.time()
                    elapsed = current_time - start_time
                    
                    # è§£æè¿›åº¦ä¿¡æ¯
                    if 'completed' in output.lower() and 'queries' in output.lower():
                        try:
                            # å°è¯•æå–å®Œæˆæ•°é‡
                            if '/' in output:
                                parts = output.split('/')
                                if len(parts) >= 2:
                                    completed = parts[0].split()[-1]
                                    total = parts[1].split()[0]
                                    if completed.isdigit() and total.isdigit():
                                        progress = int(completed) / int(total) * 100
                                        print(f"   ğŸ“ˆ Progress: {completed}/{total} ({progress:.1f}%) - {elapsed:.1f}s elapsed")
                        except:
                            pass
                    
                    # æ˜¾ç¤ºå…¶ä»–é‡è¦ä¿¡æ¯
                    if 'progress' in output.lower() or 'completed' in output.lower():
                        print(f"   ğŸ“Š {output}")
                    
                    last_progress_time = current_time
                
                # æ˜¾ç¤ºé”™è¯¯ä¿¡æ¯
                elif 'error' in output.lower() or 'failed' in output.lower():
                    print(f"   âŒ {output}")
                
                # æ˜¾ç¤ºè­¦å‘Šä¿¡æ¯
                elif 'warning' in output.lower() or 'skip' in output.lower():
                    print(f"   âš  {output}")
                
                # å®šæœŸæ£€æŸ¥è¾“å‡ºæ–‡ä»¶å¤§å°
                current_time = time.time()
                if current_time - last_file_check_time > 15:  # æ¯15ç§’æ£€æŸ¥ä¸€æ¬¡æ–‡ä»¶å¤§å°
                    if output_file.exists():
                        current_size = output_file.stat().st_size
                        if current_size > last_file_size:
                            size_mb = current_size / (1024 * 1024)
                            print(f"   ğŸ’¾ Output file: {size_mb:.1f} MB")
                            last_file_size = current_size
                    
                    last_file_check_time = current_time
                
                # å®šæœŸæ˜¾ç¤ºçŠ¶æ€ï¼ˆå¦‚æœæ²¡æœ‰å…¶ä»–è¾“å‡ºï¼‰
                elif time.time() - last_progress_time > 30:  # 30ç§’æ— è¾“å‡ºæ—¶æ˜¾ç¤ºçŠ¶æ€
                    elapsed = time.time() - start_time
                    print(f"   â³ Still running... ({elapsed:.1f}s elapsed)")
                    last_progress_time = time.time()
        
        # ç­‰å¾…è¿›ç¨‹å®Œæˆ
        return_code = process.wait()
        
        if return_code == 0:
            total_time = time.time() - start_time
            
            # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶
            if output_file.exists():
                size = output_file.stat().st_size
                size_mb = size / (1024 * 1024)
                print(f"   ğŸ’¾ Output file: {size_mb:.1f} MB")
                
                try:
                    with open(output_file, 'r', encoding='utf-8') as f:
                        line_count = sum(1 for _ in f)
                    print(f"   ğŸ“Š Output file lines: {line_count}")
                except Exception as e:
                    print(f"   âš  Could not count file lines: {e}")
            else:
                print(f"   âŒ Output file not found!")
                return False
            
            print(f"âœ“ History generation completed for {data_type} in {total_time:.1f}s")
            return True
        else:
            print(f"âŒ History generation failed for {data_type} (exit code: {return_code})")
            return False
            
    except Exception as e:
        print(f"âŒ Error during history generation for {data_type}: {e}")
        return False

def run_evaluation(data_type: str):
    """è¿è¡Œè¯„ä¼°"""
    if data_type == "prin":
        evaluator_script = PROJECT_ROOT / "Evaluator" / "prin_risk_eval.py"
        history_file = HISTORY_DIR / f"histories_prin_{os.getenv('MODEL', 'unknown')}.jsonl"
        output_file = EVALUATION_DIR / f"prin_eval_results_{os.getenv('MODEL', 'unknown')}.jsonl"
    else:
        evaluator_script = PROJECT_ROOT / "Evaluator" / "env_risk_eval.py"
        history_file = HISTORY_DIR / f"histories_env_{os.getenv('MODEL', 'unknown')}.jsonl"
        output_file = EVALUATION_DIR / f"env_eval_results_{os.getenv('MODEL', 'unknown')}.jsonl"
    
    if not evaluator_script.exists():
        print(f"âŒ Evaluator script not found: {evaluator_script}")
        return False
    
    if not history_file.exists():
        print(f"âŒ History file not found: {history_file}")
        return False
    
    print(f"ğŸ” Running evaluation for {data_type} data...")
    print(f"ğŸ“ Input: {history_file}")
    print(f"ğŸ“ Output: {output_file}")
    
    try:
        cmd = [
            sys.executable, 
            str(evaluator_script),
            "--input", str(history_file),
            "--output", str(output_file)
        ]
        result = subprocess.run(
            cmd,
            cwd=PROJECT_ROOT,
            capture_output=True,
            text=True,
            timeout=300
        )
        
        if result.returncode == 0:
            print(f"âœ“ Evaluation completed for {data_type}")
            if output_file.exists():
                print(f"ğŸ“Š Results saved to: {output_file}")
            return True
        else:
            print(f"âŒ Evaluation failed for {data_type}")
            if result.stderr:
                print(f"Error details: {result.stderr}")
            return False
            
    except Exception as e:
        print(f"âŒ Error during evaluation for {data_type}: {e}")
        return False

def process_model(model: str, max_workers: int = 10, debug: bool = False, progress_monitor=None, history_gen_only: bool = False, evaluation_only: bool = False):
    """å¤„ç†å•ä¸ªæ¨¡å‹çš„å®Œæ•´æµç¨‹"""
    if progress_monitor:
        progress_monitor.update_model(model)
    
    print(f"\n{'='*60}")
    print(f"ğŸš€ Processing model: {model}")
    if history_gen_only:
        print(f"ğŸ“ Mode: History Generation Only (No Evaluation)")
    elif evaluation_only:
        print(f"ğŸ” Mode: Evaluation Only (Skip History Generation)")
    else:
        print(f"ğŸ”„ Mode: Full Pipeline (History Generation + Evaluation)")
    print(f"{'='*60}")
    
    if not update_env_model(model):
        return False
    
    
    results = {}
    
    # å¤„ç†ENVæ•°æ®
    print(f"\nğŸ“Š Processing ENV data...")
    if evaluation_only:
        # åªåševaluationï¼Œè·³è¿‡history generation
        env_success = None
        env_eval_success = run_evaluation("env")
        results["env"] = {
            "history_generation": None,
            "evaluation": env_eval_success
        }
    else:
        # åšhistory generation
        env_success = run_history_generation("env", max_workers, debug)
        if env_success and not history_gen_only:
            env_eval_success = run_evaluation("env")
            results["env"] = {
                "history_generation": env_success,
                "evaluation": env_eval_success
            }
        else:
            results["env"] = {
                "history_generation": env_success,
                "evaluation": None if history_gen_only else False
            }
    
    return results

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='Automated model evaluation pipeline')
    parser.add_argument('--max-workers', type=int, default=50, 
                       help='Maximum number of concurrent workers')
    parser.add_argument('--debug', action='store_true', 
                       help='Enable debug mode')
    parser.add_argument('--models', nargs='+', 
                       help='Specific models to process')
    parser.add_argument('--show-progress', action='store_true',
                       help='Show detailed progress monitoring')
    parser.add_argument('--history-gen-only', action='store_true',
                       help='Only run history generation, skip evaluation')
    parser.add_argument('--evaluation-only', action='store_true',
                       help='Only run evaluation, skip history generation')
    parser.add_argument('--list-models', action='store_true',
                       help='List all available models and exit')
    
    args = parser.parse_args()
    
    # å¦‚æœç”¨æˆ·è¯·æ±‚åˆ—å‡ºæ¨¡å‹ï¼Œæ˜¾ç¤ºå¹¶é€€å‡º
    if args.list_models:
        print("Available models:")
        print("\nOnline models:")
        for model in get_online_models():
            print(f"  - {model}")
        print("\nLocal models:")
        for model in get_local_models():
            model_info = MODEL_INFO[model]
            print(f"  - {model}")
            print(f"    Model path: {model_info.get('model_path', 'None')}")
            print(f"    Tokenizer path: {model_info.get('tokenizer_path', 'None')}")
        sys.exit(0)
    
    # æ£€æŸ¥å‚æ•°å†²çª
    if args.history_gen_only and args.evaluation_only:
        print("âŒ Error: Cannot use both --history-gen-only and --evaluation-only")
        print("Please choose one mode or use default (full pipeline)")
        sys.exit(1)
    
    # æ ¹æ®æ¨¡å¼æ˜¾ç¤ºä¸åŒçš„å¯åŠ¨ä¿¡æ¯
    if args.history_gen_only:
        print("ğŸš€ Starting automated history generation pipeline...")
        print("ğŸ“ Mode: History Generation Only (No Evaluation)")
    elif args.evaluation_only:
        print("ğŸš€ Starting automated evaluation pipeline...")
        print("ğŸ” Mode: Evaluation Only (Skip History Generation)")
    else:
        print("ğŸš€ Starting automated model evaluation pipeline...")
        print("ğŸ”„ Mode: Full Pipeline (History Generation + Evaluation)")
    
    print(f"Project root: {PROJECT_ROOT}")
    print(f"Output directory: {OUTPUT_DIR}")
    
    load_dotenv_safe()
    
    models_to_process = args.models if args.models else DEFAULT_MODELS
    
    # éªŒè¯æ‰€æœ‰æ¨¡å‹éƒ½åœ¨MODEL_INFOä¸­å­˜åœ¨
    invalid_models = [model for model in models_to_process if model not in MODEL_INFO]
    if invalid_models:
        print(f"âŒ Error: The following models are not found in MODEL_INFO: {invalid_models}")
        print(f"Available models: {list(MODEL_INFO.keys())}")
        sys.exit(1)
    
    print(f"Models to process: {models_to_process}")
    
    OUTPUT_DIR.mkdir(exist_ok=True)
    
    # åˆå§‹åŒ–è¿›åº¦ç›‘æ§å™¨
    progress_monitor = None
    if args.show_progress:
        progress_monitor = ProgressMonitor(len(models_to_process))
        print(f"ğŸ“Š Progress monitoring enabled")
    
    all_results = {}
    start_time = time.time()
    
    for i, model in enumerate(models_to_process, 1):
        print(f"\nğŸ“‹ Progress: {i}/{len(models_to_process)}")
        try:
            results = process_model(model, args.max_workers, args.debug, progress_monitor, args.history_gen_only, args.evaluation_only)
            all_results[model] = results
        except Exception as e:
            print(f"âŒ Critical error processing model {model}: {e}")
            all_results[model] = {"error": str(e)}
        
    total_time = time.time() - start_time
    summary = {
        "total_time_seconds": total_time,
        "total_time_hours": total_time / 3600,
        "models_processed": len(models_to_process),
        "mode": "history_generation_only" if args.history_gen_only else "evaluation_only" if args.evaluation_only else "full_pipeline",
        "results": all_results
    }
    
    summary_file = OUTPUT_DIR / "pipeline_summary.json"
    try:
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        print(f"\nâœ“ Pipeline summary saved to: {summary_file}")
    except Exception as e:
        print(f"âš  Warning: Could not save summary: {e}")
    
    print(f"\n{'='*60}")
    if args.history_gen_only:
        print(f"ğŸ‰ History Generation Pipeline completed!")
        print(f"ğŸ“ Mode: History Generation Only (No Evaluation)")
    elif args.evaluation_only:
        print(f"ğŸ‰ Evaluation Pipeline completed!")
        print(f"ğŸ” Mode: Evaluation Only (Skip History Generation)")
    else:
        print(f"ğŸ‰ Full Pipeline completed!")
        print(f"ğŸ”„ Mode: History Generation + Evaluation")
    print(f"Total time: {total_time/3600:.2f} hours")
    print(f"Models processed: {len(models_to_process)}")
    print(f"Results saved to:")
    print(f"  ğŸ“ History files: {HISTORY_DIR}")
    print(f"  ğŸ“ Evaluation files: {EVALUATION_DIR}")
    print(f"  ğŸ“ Summary file: {OUTPUT_DIR}")
    print(f"{'='*60}")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nâš  Pipeline interrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ Critical error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)